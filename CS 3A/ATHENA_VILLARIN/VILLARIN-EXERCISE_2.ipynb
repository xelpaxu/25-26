{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ce1870",
   "metadata": {},
   "source": [
    "# Exercise for Unit 2\n",
    "\n",
    "**Name:** Athena S. Villarin  \n",
    "**Date:** 9/12/2025  \n",
    "**Year and Section:** BSCS 3-A AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618e901",
   "metadata": {},
   "source": [
    "## Implementation of Dense_Layer Class\n",
    "\n",
    "This class implements:\n",
    "- Setup inputs and weights  \n",
    "- Weighted sum + bias  \n",
    "- Activation functions (ReLU, Sigmoid, Softmax)  \n",
    "- Loss functions (MSE, (Cross-)Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c7bf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Dense_Layer:\n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.output = None\n",
    "\n",
    "    # (a) Function to setup/accept the inputs and weights\n",
    "    def setup_inputs_and_weights(self, inputs, weights, biases):\n",
    "        self.inputs = np.array(inputs)\n",
    "        self.weights = np.array(weights)\n",
    "        self.biases = np.array(biases)\n",
    "\n",
    "    # (b) Function to perform weighted sum + bias\n",
    "    def weighted_sum(self):\n",
    "        inputs_col = self.inputs.reshape(-1, 1)\n",
    "        result = np.dot(self.weights, inputs_col).flatten() + self.biases\n",
    "        return result\n",
    "\n",
    "    # (c) Function to perform activation function\n",
    "    def activation(self, z, function=\"relu\"):\n",
    "        if function.lower() == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif function.lower() == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif function.lower() == \"softmax\":\n",
    "            exp_vals = np.exp(z - np.max(z))\n",
    "            return exp_vals / np.sum(exp_vals)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    # (d) Function to calculate loss (Cross-Entropy). Note: At the bottom, sir, I added manual calculation\n",
    "    def loss(self, predicted, target):\n",
    "        predicted = np.array(predicted)\n",
    "        target = np.array(target)\n",
    "        \n",
    "        epsilon = 1e-12  # to avoid log(0)\n",
    "        predicted = np.clip(predicted, epsilon, 1. - epsilon)\n",
    "        \n",
    "        # Binary classification (single output)\n",
    "        if len(predicted) == 1 and len(target) == 1:\n",
    "            return -(target[0] * np.log(predicted[0]) + (1 - target[0]) * np.log(1 - predicted[0]))\n",
    "        # Multi-class classification\n",
    "        else:\n",
    "            return -np.sum(target * np.log(predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d2084",
   "metadata": {},
   "source": [
    "## Problem 2a – Iris Dataset (Forward Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51117e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 Weighted Sum (z1): [3.93 0.15 0.85]\n",
      "Layer 1 Activated Output (a1): [3.93 0.15 0.85]\n",
      "\n",
      "Layer 2 Weighted Sum (z2): [5.074 4.805]\n",
      "Layer 2 Activated Output (a2): [0.99378157 0.99187781]\n",
      "\n",
      "Output Layer Weighted Sum (z3): [-1.20148478  2.39699221 -2.90172587]\n",
      "Final Output (a3, Softmax probabilities): [0.0265075  0.96865119 0.00484132]\n",
      "\n",
      "Cross-Entropy Loss: 3.080656405230887\n",
      "Predicted Class: Iris-versicolor (1)\n"
     ]
    }
   ],
   "source": [
    "# Problem 2a: Iris Dataset Forward Propagation\n",
    "\n",
    "# Inputs\n",
    "X = [5.1, 3.5, 1.4, 0.2]\n",
    "Target_output = [0.7, 0.2, 0.1]\n",
    "\n",
    "# First Hidden Layer\n",
    "layer1 = Dense_Layer()\n",
    "W1 = [[0.2, 0.1, -0.4, 0.6],\n",
    "      [0.5, -0.2, 0.3, -0.1],\n",
    "      [-0.3, 0.4, 0.2, 0.5]]\n",
    "B1 = [3.0, -2.1, 0.6]\n",
    "\n",
    "layer1.setup_inputs_and_weights(X, W1, B1)\n",
    "z1 = layer1.weighted_sum()\n",
    "a1 = layer1.activation(z1, \"relu\")\n",
    "print(\"Layer 1 Weighted Sum (z1):\", z1)\n",
    "print(\"Layer 1 Activated Output (a1):\", a1)\n",
    "\n",
    "# Second Hidden Layer\n",
    "layer2 = Dense_Layer()\n",
    "W2 = [[0.3, 0.7, -0.6],\n",
    "      [-0.5, 0.2, 0.4]]\n",
    "B2 = [4.3, 6.4]\n",
    "\n",
    "layer2.setup_inputs_and_weights(a1, W2, B2)\n",
    "z2 = layer2.weighted_sum()\n",
    "a2 = layer2.activation(z2, \"sigmoid\")\n",
    "print(\"\\nLayer 2 Weighted Sum (z2):\", z2)\n",
    "print(\"Layer 2 Activated Output (a2):\", a2)\n",
    "\n",
    "# Output Layer\n",
    "layer3 = Dense_Layer()\n",
    "W3 = [[0.5, -0.2],\n",
    "      [-0.3, 0.6],\n",
    "      [0.8, -0.4]]\n",
    "B3 = [-1.5, 2.1, -3.3]\n",
    "\n",
    "layer3.setup_inputs_and_weights(a2, W3, B3)\n",
    "z3 = layer3.weighted_sum()\n",
    "a3 = layer3.activation(z3, \"softmax\")\n",
    "print(\"\\nOutput Layer Weighted Sum (z3):\", z3)\n",
    "print(\"Final Output (a3, Softmax probabilities):\", a3)\n",
    "\n",
    "# Loss Calculation\n",
    "loss_value = layer3.loss(a3, Target_output)\n",
    "print(\"\\nCross-Entropy Loss:\", loss_value)\n",
    "\n",
    "# Predicted Class\n",
    "classes = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "predicted_index = np.argmax(a3)\n",
    "predicted_class = classes[predicted_index]\n",
    "print(\"Predicted Class:\", predicted_class, f\"({predicted_index})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132d00a",
   "metadata": {},
   "source": [
    "## Problem 2b – Breast Cancer Dataset (Forward Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f65243e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 Weighted Sum (z1): [11.3435  3.4755 -0.2905]\n",
      "Layer 1 Activated Output (a1): [11.3435  3.4755  0.    ]\n",
      "\n",
      "Layer 2 Weighted Sum (z2): [ 6.211  -2.4653]\n",
      "Layer 2 Activated Output (a2): [0.99799679 0.07832686]\n",
      "\n",
      "Output Layer Weighted Sum (z3): [0.85943432]\n",
      "Final Output (a3, Probability of Malignant): [0.70254245]\n",
      "\n",
      "Cross-Entropy Loss: 0.35304944627096974\n",
      "Predicted Class: Malignant (1)\n"
     ]
    }
   ],
   "source": [
    "# Problem 2b: Breast Cancer Dataset Forward Propagation\n",
    "\n",
    "# Inputs\n",
    "X = [14.1, 20.3, 0.095]\n",
    "Target_output = [1]   # Malignant\n",
    "\n",
    "# First Hidden Layer\n",
    "layer1 = Dense_Layer()\n",
    "W1 = [[0.5, 0.2, -0.7],\n",
    "      [-0.3, 0.4, 0.9],\n",
    "      [0.8, -0.6, 0.1]]\n",
    "B1 = [0.3, -0.5, 0.6]\n",
    "\n",
    "layer1.setup_inputs_and_weights(X, W1, B1)\n",
    "z1 = layer1.weighted_sum()\n",
    "a1 = layer1.activation(z1, \"relu\")\n",
    "print(\"Layer 1 Weighted Sum (z1):\", z1)\n",
    "print(\"Layer 1 Activated Output (a1):\", a1)\n",
    "\n",
    "# Second Hidden Layer\n",
    "layer2 = Dense_Layer()\n",
    "W2 = [[0.6, -0.2, 0.4],\n",
    "      [-0.3, 0.5, 0.7]]\n",
    "B2 = [0.1, -0.8]\n",
    "\n",
    "layer2.setup_inputs_and_weights(a1, W2, B2)\n",
    "z2 = layer2.weighted_sum()\n",
    "a2 = layer2.activation(z2, \"sigmoid\")\n",
    "print(\"\\nLayer 2 Weighted Sum (z2):\", z2)\n",
    "print(\"Layer 2 Activated Output (a2):\", a2)\n",
    "\n",
    "# Output Layer\n",
    "layer3 = Dense_Layer()\n",
    "W3 = [[0.7, -0.5]]\n",
    "B3 = [0.2]\n",
    "\n",
    "layer3.setup_inputs_and_weights(a2, W3, B3)\n",
    "z3 = layer3.weighted_sum()\n",
    "a3 = layer3.activation(z3, \"sigmoid\")\n",
    "print(\"\\nOutput Layer Weighted Sum (z3):\", z3)\n",
    "print(\"Final Output (a3, Probability of Malignant):\", a3)\n",
    "\n",
    "# Loss Calculation\n",
    "loss_value = layer3.loss(a3, Target_output)\n",
    "print(\"\\nCross-Entropy Loss:\", loss_value)\n",
    "\n",
    "# Classification\n",
    "predicted_label = 1 if a3 >= 0.5 else 0\n",
    "classification = \"Malignant (1)\" if predicted_label == 1 else \"Benign (0)\"\n",
    "print(\"Predicted Class:\", classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266cd9da",
   "metadata": {},
   "source": [
    "### Additional: Manual Cross-Entropy Loss (like in your video, sir, if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0678a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Loss (Problem 2a): 3.080656224018649\n",
      "Predicted Class: Iris-versicolor (1)\n"
     ]
    }
   ],
   "source": [
    "softmax_output = [0.0265075, 0.96865119, 0.00484132]   # predicted\n",
    "target_output = [0.7, 0.2, 0.1]                        # target\n",
    "\n",
    "# Long manual formula\n",
    "loss_long = -(math.log(softmax_output[0]) * target_output[0] +\n",
    "              math.log(softmax_output[1]) * target_output[1] +\n",
    "              math.log(softmax_output[2]) * target_output[2])\n",
    "\n",
    "print(\"Manual Loss (Problem 2a):\", loss_long)\n",
    "\n",
    "# Predicted Class\n",
    "classes = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "predicted_index = np.argmax(softmax_output)\n",
    "predicted_class = classes[predicted_index]\n",
    "print(\"Predicted Class:\", predicted_class, f\"({predicted_index})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb2e52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Loss (Problem 2b): 0.35304945255361325\n",
      "Predicted Class: Malignant (1)\n"
     ]
    }
   ],
   "source": [
    "sigmoid_output = 0.70254245   # actual final probability\n",
    "target_output = 1             # malignant\n",
    "\n",
    "# Long manual formula for binary cross-entropy\n",
    "loss_long = -(target_output * math.log(sigmoid_output) +\n",
    "              (1 - target_output) * math.log(1 - sigmoid_output))\n",
    "\n",
    "print(\"Manual Loss (Problem 2b):\", loss_long)\n",
    "\n",
    "# Classification\n",
    "predicted_label = 1 if sigmoid_output >= 0.5 else 0   # 1 = Malignant, 0 = Benign\n",
    "classification = \"Malignant (1)\" if predicted_label == 1 else \"Benign (0)\"\n",
    "\n",
    "print(\"Predicted Class:\", classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679b710",
   "metadata": {},
   "source": [
    "Added this line for git to register changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
