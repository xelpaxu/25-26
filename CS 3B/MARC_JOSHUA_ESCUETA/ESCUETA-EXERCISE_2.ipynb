{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2e68f6",
   "metadata": {},
   "source": [
    "# Number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edad2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    \"\"\"\n",
    "    Dense Layer class for neural networks without using any imports\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Dense Layer\"\"\"\n",
    "        self.inputs = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.weighted_sum = None\n",
    "        self.output = None\n",
    "        self.activation_type = None\n",
    "    \n",
    "    # a) Function to setup/accept the inputs and weights\n",
    "    def setup(self, inputs, weights, bias):\n",
    "        \"\"\"\n",
    "        Setup and accept inputs, weights, and bias\n",
    "        \n",
    "        Args:\n",
    "            inputs: list of input values\n",
    "            weights: 2D list (matrix) of weights\n",
    "            bias: list of bias values\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        print(f\"Layer configured:\")\n",
    "        print(f\"  Inputs: {inputs}\")\n",
    "        print(f\"  Weights shape: {len(weights)}x{len(weights[0]) if weights else 0}\")\n",
    "        print(f\"  Bias: {bias}\")\n",
    "    \n",
    "    # b) Function to perform the weighted sum + bias\n",
    "    def calculate_weighted_sum(self):\n",
    "        \"\"\"\n",
    "        Calculate weighted sum: Z = W * X + b\n",
    "        Performs matrix multiplication manually\n",
    "        \n",
    "        Returns:\n",
    "            List of weighted sum values\n",
    "        \"\"\"\n",
    "        if self.inputs is None or self.weights is None or self.bias is None:\n",
    "            raise ValueError(\"Must call setup() first\")\n",
    "        \n",
    "        # Number of neurons (rows in weight matrix)\n",
    "        num_neurons = len(self.weights)\n",
    "        \n",
    "        # Initialize result\n",
    "        result = []\n",
    "        \n",
    "        # For each neuron\n",
    "        for i in range(num_neurons):\n",
    "            weighted_sum = 0\n",
    "            # Multiply weights by inputs\n",
    "            for j in range(len(self.inputs)):\n",
    "                weighted_sum += self.weights[i][j] * self.inputs[j]\n",
    "            # Add bias\n",
    "            weighted_sum += self.bias[i]\n",
    "            result.append(weighted_sum)\n",
    "        \n",
    "        self.weighted_sum = result\n",
    "        print(f\"\\nWeighted Sum (Z): {result}\")\n",
    "        return result\n",
    "    \n",
    "    # c) Function to perform the selected activation function\n",
    "    def activation(self, activation_type):\n",
    "        \"\"\"\n",
    "        Apply activation function to weighted sum\n",
    "        \n",
    "        Args:\n",
    "            activation_type: string - 'relu', 'sigmoid', 'softmax', 'linear', 'step'\n",
    "        \n",
    "        Returns:\n",
    "            List of activated values\n",
    "        \"\"\"\n",
    "        if self.weighted_sum is None:\n",
    "            raise ValueError(\"Must call calculate_weighted_sum() first\")\n",
    "        \n",
    "        self.activation_type = activation_type\n",
    "        \n",
    "        if activation_type.lower() == 'relu':\n",
    "            self.output = self._relu(self.weighted_sum)\n",
    "        elif activation_type.lower() == 'sigmoid':\n",
    "            self.output = self._sigmoid(self.weighted_sum)\n",
    "        elif activation_type.lower() == 'softmax':\n",
    "            self.output = self._softmax(self.weighted_sum)\n",
    "        elif activation_type.lower() == 'linear':\n",
    "            self.output = self._linear(self.weighted_sum)\n",
    "        elif activation_type.lower() == 'step':\n",
    "            self.output = self._step(self.weighted_sum)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {activation_type}\")\n",
    "        \n",
    "        print(f\"Activation ({activation_type}): {self.output}\")\n",
    "        return self.output\n",
    "    \n",
    "    def _relu(self, values):\n",
    "        \"\"\"ReLU activation: max(0, x)\"\"\"\n",
    "        return [max(0, x) for x in values]\n",
    "    \n",
    "    def _sigmoid(self, values):\n",
    "        \"\"\"Sigmoid activation: 1 / (1 + e^(-x))\"\"\"\n",
    "        result = []\n",
    "        for x in values:\n",
    "            # Calculate e^(-x) using Taylor series approximation\n",
    "            exp_neg_x = self._exp(-x)\n",
    "            sigmoid_value = 1 / (1 + exp_neg_x)\n",
    "            result.append(sigmoid_value)\n",
    "        return result\n",
    "    \n",
    "    def _softmax(self, values):\n",
    "        \"\"\"Softmax activation: e^(xi) / sum(e^(xj))\"\"\"\n",
    "        # Calculate e^x for all values\n",
    "        exp_values = [self._exp(x) for x in values]\n",
    "        # Sum of all exponentials\n",
    "        sum_exp = sum(exp_values)\n",
    "        # Normalize\n",
    "        return [exp_val / sum_exp for exp_val in exp_values]\n",
    "    \n",
    "    def _linear(self, values):\n",
    "        \"\"\"Linear activation: f(x) = x\"\"\"\n",
    "        return values\n",
    "    \n",
    "    def _step(self, values, threshold=0):\n",
    "        \"\"\"Step activation: 1 if x >= threshold, else 0\"\"\"\n",
    "        return [1 if x >= threshold else 0 for x in values]\n",
    "    \n",
    "    def _exp(self, x):\n",
    "        \"\"\"\n",
    "        Calculate e^x using Taylor series expansion\n",
    "        e^x = 1 + x + x^2/2! + x^3/3! + x^4/4! + ...\n",
    "        \"\"\"\n",
    "        # For numerical stability, limit x\n",
    "        if x > 20:\n",
    "            x = 20\n",
    "        elif x < -20:\n",
    "            x = -20\n",
    "        \n",
    "        result = 1.0\n",
    "        term = 1.0\n",
    "        \n",
    "        # Use 20 terms for good approximation\n",
    "        for n in range(1, 20):\n",
    "            term *= x / n\n",
    "            result += term\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # d) Function to calculate the loss\n",
    "    def calculate_loss(self, target_output, loss_type='mse'):\n",
    "        \"\"\"\n",
    "        Calculate loss between predicted output and target output\n",
    "        \n",
    "        Args:\n",
    "            target_output: list of target values\n",
    "            loss_type: 'mse', 'mae', 'binary_crossentropy', 'categorical_crossentropy'\n",
    "        \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        if self.output is None:\n",
    "            raise ValueError(\"Must call activation() first\")\n",
    "        \n",
    "        if len(self.output) != len(target_output):\n",
    "            raise ValueError(\"Output and target must have same length\")\n",
    "        \n",
    "        if loss_type.lower() == 'mse':\n",
    "            loss = self._mse(self.output, target_output)\n",
    "        elif loss_type.lower() == 'mae':\n",
    "            loss = self._mae(self.output, target_output)\n",
    "        elif loss_type.lower() == 'binary_crossentropy':\n",
    "            loss = self._binary_crossentropy(self.output, target_output)\n",
    "        elif loss_type.lower() == 'categorical_crossentropy':\n",
    "            loss = self._categorical_crossentropy(self.output, target_output)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function: {loss_type}\")\n",
    "        \n",
    "        print(f\"\\nLoss ({loss_type}): {loss}\")\n",
    "        return loss\n",
    "    \n",
    "    def _mse(self, predicted, target):\n",
    "        \"\"\"Mean Squared Error\"\"\"\n",
    "        squared_errors = [(p - t) ** 2 for p, t in zip(predicted, target)]\n",
    "        return sum(squared_errors) / len(squared_errors)\n",
    "    \n",
    "    def _mae(self, predicted, target):\n",
    "        \"\"\"Mean Absolute Error\"\"\"\n",
    "        absolute_errors = [abs(p - t) for p, t in zip(predicted, target)]\n",
    "        return sum(absolute_errors) / len(absolute_errors)\n",
    "    \n",
    "    def _binary_crossentropy(self, predicted, target):\n",
    "        \"\"\"Binary Cross Entropy Loss\"\"\"\n",
    "        epsilon = 1e-7  # Small value to avoid log(0)\n",
    "        loss = 0\n",
    "        for p, t in zip(predicted, target):\n",
    "            p = max(min(p, 1 - epsilon), epsilon)  # Clip values\n",
    "            loss += -(t * self._log(p) + (1 - t) * self._log(1 - p))\n",
    "        return loss / len(predicted)\n",
    "    \n",
    "    def _categorical_crossentropy(self, predicted, target):\n",
    "        \"\"\"Categorical Cross Entropy Loss\"\"\"\n",
    "        epsilon = 1e-7\n",
    "        loss = 0\n",
    "        for p, t in zip(predicted, target):\n",
    "            p = max(p, epsilon)  # Avoid log(0)\n",
    "            loss += -t * self._log(p)\n",
    "        return loss\n",
    "    \n",
    "    def _log(self, x):\n",
    "        \"\"\"Natural logarithm using Taylor series around x=1\"\"\"\n",
    "        if x <= 0:\n",
    "            return -100  # Avoid log of non-positive\n",
    "        \n",
    "        # For x close to 1, use ln(x) = 2 * sum((((x-1)/(x+1))^(2n+1))/(2n+1))\n",
    "        # For other x, use properties: ln(x) = ln(a * b) = ln(a) + ln(b)\n",
    "        \n",
    "        # Reduce x to range [0.5, 1.5] for better convergence\n",
    "        exponent = 0\n",
    "        while x > 1.5:\n",
    "            x /= 2.718281828459045  # e\n",
    "            exponent += 1\n",
    "        while x < 0.5:\n",
    "            x *= 2.718281828459045  # e\n",
    "            exponent -= 1\n",
    "        \n",
    "        # Taylor series for ln(x) around 1: ln(x) = (x-1) - (x-1)^2/2 + (x-1)^3/3 - ...\n",
    "        y = x - 1\n",
    "        result = 0\n",
    "        term = y\n",
    "        \n",
    "        for n in range(1, 50):\n",
    "            result += term / n\n",
    "            term *= -y\n",
    "        \n",
    "        return result + exponent\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "print(\"=\" * 60)\n",
    "print(\"DENSE LAYER CLASS - TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test example with simple data\n",
    "layer1 = Dense_Layer()\n",
    "\n",
    "# Setup inputs and weights\n",
    "inputs = [1.0, 2.0, 3.0]\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5],\n",
    "    [0.5, -0.91, 0.26]\n",
    "]\n",
    "bias = [2.0, 3.0]\n",
    "\n",
    "layer1.setup(inputs, weights, bias)\n",
    "layer1.calculate_weighted_sum()\n",
    "layer1.activation('relu')\n",
    "target = [3.0, 1.0]\n",
    "layer1.calculate_loss(target, 'mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14eb945",
   "metadata": {},
   "source": [
    "# Number 2 a.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Dense_Layer class from Problem 1\n",
    "\n",
    "# Initialize layers\n",
    "hidden_layer_1 = Dense_Layer()\n",
    "hidden_layer_2 = Dense_Layer()\n",
    "output_layer = Dense_Layer()\n",
    "\n",
    "# Input data\n",
    "inputs = [5.1, 3.5, 1.4, 0.2]\n",
    "target_output = [0.7, 0.2, 0.1]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"IRIS DATASET CLASSIFICATION - NEURAL NETWORK FORWARD PASS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "#  FIRST HIDDEN LAYER (ReLU) \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LAYER 1 - FIRST HIDDEN LAYER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "W1 = [\n",
    "    [0.2, 0.5, -0.3, 0.1],\n",
    "    [-0.2, 0.4, -0.4, 0.3],\n",
    "    [0.2, 0.6, -0.1, 0.5]\n",
    "]\n",
    "B1 = [3.0, -2.1, 0.6]\n",
    "\n",
    "hidden_layer_1.setup(inputs, W1, B1)\n",
    "z1 = hidden_layer_1.calculate_weighted_sum()\n",
    "a1 = hidden_layer_1.activation('relu')\n",
    "\n",
    "#  SECOND HIDDEN LAYER (Sigmoid) \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LAYER 2 - SECOND HIDDEN LAYER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "W2 = [\n",
    "    [0.3, -0.5, 0.7],\n",
    "    [0.2, -0.6, 0.4]\n",
    "]\n",
    "B2 = [4.3, 6.4]\n",
    "\n",
    "hidden_layer_2.setup(a1, W2, B2)\n",
    "z2 = hidden_layer_2.calculate_weighted_sum()\n",
    "a2 = hidden_layer_2.activation('sigmoid')\n",
    "\n",
    "#  OUTPUT LAYER (Softmax) \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LAYER 3 - OUTPUT LAYER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "W3 = [\n",
    "    [0.5, -0.3],\n",
    "    [0.8, -0.2],\n",
    "    [0.6, -0.4]\n",
    "]\n",
    "B3 = [-1.5, 2.1, -3.3]\n",
    "\n",
    "output_layer.setup(a2, W3, B3)\n",
    "z3 = output_layer.calculate_weighted_sum()\n",
    "predictions = output_layer.activation('softmax')\n",
    "\n",
    "#  CALCULATE LOSS \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOSS CALCULATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nPredicted Output: {predictions}\")\n",
    "print(f\"Target Output: {target_output}\")\n",
    "\n",
    "loss = output_layer.calculate_loss(target_output, 'categorical_crossentropy')\n",
    "\n",
    "#  FINAL RESULTS \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "species = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "print(f\"\\n{'Class':<20} {'Probability':<15} {'Confidence %':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "max_prob = max(predictions)\n",
    "max_index = predictions.index(max_prob)\n",
    "\n",
    "for i, prob in enumerate(predictions):\n",
    "    print(f\"{species[i]:<20} {prob:<15.6f} {prob*100:<15.2f}%\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nPredicted Class: {species[max_index]}\")\n",
    "print(f\"Confidence: {max_prob*100:.2f}%\")\n",
    "print(f\"Categorical Cross Entropy Loss: {loss:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef777412",
   "metadata": {},
   "source": [
    "# Number 2 b.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b2c1774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "BREAST CANCER DATASET CLASSIFICATION - NEURAL NETWORK FORWARD PASS\n",
      "===========================================================================\n",
      "\n",
      "Input Features:\n",
      "  Mean Radius: 14.1\n",
      "  Mean Texture: 20.3\n",
      "  Mean Smoothness: 0.095\n",
      "Target Output: 1 (Malignant)\n",
      "\n",
      "===========================================================================\n",
      "LAYER 1 - FIRST HIDDEN LAYER (ReLU Activation)\n",
      "===========================================================================\n",
      "\n",
      "Weights W1:\n",
      "  Node 1: [0.5, -0.3, 0.8]\n",
      "  Node 2: [0.2, 0.4, -0.6]\n",
      "  Node 3: [-0.7, 0.9, 0.1]\n",
      "Bias B1: [0.3, -0.5, 0.6]\n",
      "\n",
      "Weighted Sum (Z1):\n",
      "  Node 1: 1.3360\n",
      "    Calculation: (0.5×14.1) + (-0.3×20.3) + (0.8×0.095) + 0.3\n",
      "  Node 2: 10.3830\n",
      "    Calculation: (0.2×14.1) + (0.4×20.3) + (-0.6×0.095) + -0.5\n",
      "  Node 3: 9.0095\n",
      "    Calculation: (-0.7×14.1) + (0.9×20.3) + (0.1×0.095) + 0.6\n",
      "\n",
      "After ReLU Activation (A1): [1.336, 10.383000000000001, 9.0095]\n",
      "\n",
      "===========================================================================\n",
      "LAYER 2 - SECOND HIDDEN LAYER (Sigmoid Activation)\n",
      "===========================================================================\n",
      "\n",
      "Weights W2:\n",
      "  Node 1: [0.6, -0.2, 0.4]\n",
      "  Node 2: [-0.3, 0.5, 0.7]\n",
      "Bias B2: [0.1, -0.8]\n",
      "\n",
      "Weighted Sum (Z2):\n",
      "  Node 1: 2.4288\n",
      "    Calculation: (0.6×1.3360) + (-0.2×10.3830) + (0.4×9.0095) + 0.1\n",
      "  Node 2: 10.2973\n",
      "    Calculation: (-0.3×1.3360) + (0.5×10.3830) + (0.7×9.0095) + -0.8\n",
      "\n",
      "After Sigmoid Activation (A2):\n",
      "  Node 1: 0.918997\n",
      "  Node 2: -0.020704\n",
      "\n",
      "===========================================================================\n",
      "LAYER 3 - OUTPUT LAYER (Sigmoid Activation)\n",
      "===========================================================================\n",
      "\n",
      "Weights W3:\n",
      "  Output Node: [0.7, -0.5]\n",
      "Bias B3: [0.2]\n",
      "\n",
      "Weighted Sum (Z3):\n",
      "  Output Node: 0.8536\n",
      "    Calculation: (0.7×0.918997) + (-0.5×-0.020704) + 0.2\n",
      "\n",
      "After Sigmoid Activation (Predictions): 0.701332\n",
      "\n",
      "===========================================================================\n",
      "LOSS CALCULATION\n",
      "===========================================================================\n",
      "\n",
      "Predicted Output: 0.701332\n",
      "Target Output: 1\n",
      "Binary Cross Entropy Loss: 0.354774\n",
      "\n",
      "===========================================================================\n",
      "CLASSIFICATION RESULT\n",
      "===========================================================================\n",
      "\n",
      "Probability Malignant (1): 0.701332 (70.13%)\n",
      "Probability Benign (0): 0.298668 (29.87%)\n",
      "Decision Threshold: 0.5\n",
      "\n",
      "Predicted Classification: MALIGNANT (1)\n",
      "Confidence: 70.13%\n",
      "\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "class Dense_Layer:\n",
    "    \"\"\"Simplified Dense Layer for forward pass\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.weighted_sum = None\n",
    "        self.output = None\n",
    "    \n",
    "    def setup(self, inputs, weights, bias):\n",
    "        \"\"\"Setup inputs, weights, and bias\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    \n",
    "    def calculate_weighted_sum(self):\n",
    "        \"\"\"Calculate Z = W * X + b\"\"\"\n",
    "        num_neurons = len(self.weights)\n",
    "        result = []\n",
    "        \n",
    "        for i in range(num_neurons):\n",
    "            weighted_sum = 0\n",
    "            for j in range(len(self.inputs)):\n",
    "                weighted_sum += self.weights[i][j] * self.inputs[j]\n",
    "            weighted_sum += self.bias[i]\n",
    "            result.append(weighted_sum)\n",
    "        \n",
    "        self.weighted_sum = result\n",
    "        return result\n",
    "    \n",
    "    def activation(self, activation_type):\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        if activation_type.lower() == 'relu':\n",
    "            self.output = [max(0, x) for x in self.weighted_sum]\n",
    "        elif activation_type.lower() == 'sigmoid':\n",
    "            self.output = [self._sigmoid(x) for x in self.weighted_sum]\n",
    "        return self.output\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Sigmoid: 1 / (1 + e^(-x))\"\"\"\n",
    "        if x > 20:\n",
    "            x = 20\n",
    "        elif x < -20:\n",
    "            x = -20\n",
    "        exp_neg_x = self._exp(-x)\n",
    "        return 1 / (1 + exp_neg_x)\n",
    "    \n",
    "    def _exp(self, x):\n",
    "        \"\"\"Calculate e^x using Taylor series\"\"\"\n",
    "        if x > 20:\n",
    "            x = 20\n",
    "        elif x < -20:\n",
    "            x = -20\n",
    "        \n",
    "        result = 1.0\n",
    "        term = 1.0\n",
    "        \n",
    "        for n in range(1, 20):\n",
    "            term *= x / n\n",
    "            result += term\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def calculate_loss(self, target_output, loss_type='binary_crossentropy'):\n",
    "        \"\"\"Calculate loss\"\"\"\n",
    "        if loss_type.lower() == 'binary_crossentropy':\n",
    "            loss = 0\n",
    "            epsilon = 1e-7\n",
    "            for p, t in zip(self.output, target_output):\n",
    "                p = max(min(p, 1 - epsilon), epsilon)\n",
    "                loss += -(t * self._log(p) + (1 - t) * self._log(1 - p))\n",
    "            return loss / len(self.output)\n",
    "    \n",
    "    def _log(self, x):\n",
    "        \"\"\"Natural logarithm approximation\"\"\"\n",
    "        if x <= 0:\n",
    "            return -100\n",
    "        \n",
    "        exponent = 0\n",
    "        while x > 1.5:\n",
    "            x /= 2.718281828459045\n",
    "            exponent += 1\n",
    "        while x < 0.5:\n",
    "            x *= 2.718281828459045\n",
    "            exponent -= 1\n",
    "        \n",
    "        y = x - 1\n",
    "        result = 0\n",
    "        term = y\n",
    "        \n",
    "        for n in range(1, 50):\n",
    "            result += term / n\n",
    "            term *= -y\n",
    "        \n",
    "        return result + exponent\n",
    "\n",
    "\n",
    "# BREAST CANCER CLASSIFICATION - NEURAL NETWORK\n",
    "\n",
    "\n",
    "print(\"=\" * 75)\n",
    "print(\"BREAST CANCER DATASET CLASSIFICATION - NEURAL NETWORK FORWARD PASS\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Initialize layers\n",
    "hidden_layer_1 = Dense_Layer()\n",
    "hidden_layer_2 = Dense_Layer()\n",
    "output_layer = Dense_Layer()\n",
    "\n",
    "# Input data\n",
    "inputs = [14.1, 20.3, 0.095]\n",
    "target_output = [1]\n",
    "\n",
    "print(\"\\nInput Features:\")\n",
    "print(f\"  Mean Radius: {inputs[0]}\")\n",
    "print(f\"  Mean Texture: {inputs[1]}\")\n",
    "print(f\"  Mean Smoothness: {inputs[2]}\")\n",
    "print(f\"Target Output: {target_output[0]} (Malignant)\")\n",
    "\n",
    "#  FIRST HIDDEN LAYER (ReLU) \n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"LAYER 1 - FIRST HIDDEN LAYER (ReLU Activation)\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "W1 = [\n",
    "    [0.5, -0.3, 0.8],\n",
    "    [0.2, 0.4, -0.6],\n",
    "    [-0.7, 0.9, 0.1]\n",
    "]\n",
    "B1 = [0.3, -0.5, 0.6]\n",
    "\n",
    "print(\"\\nWeights W1:\")\n",
    "for i, row in enumerate(W1):\n",
    "    print(f\"  Node {i+1}: {row}\")\n",
    "print(f\"Bias B1: {B1}\")\n",
    "\n",
    "hidden_layer_1.setup(inputs, W1, B1)\n",
    "z1 = hidden_layer_1.calculate_weighted_sum()\n",
    "\n",
    "print(f\"\\nWeighted Sum (Z1):\")\n",
    "for i, val in enumerate(z1):\n",
    "    calculation = f\"({W1[i][0]}×{inputs[0]}) + ({W1[i][1]}×{inputs[1]}) + ({W1[i][2]}×{inputs[2]}) + {B1[i]}\"\n",
    "    print(f\"  Node {i+1}: {val:.4f}\")\n",
    "    print(f\"    Calculation: {calculation}\")\n",
    "\n",
    "a1 = hidden_layer_1.activation('relu')\n",
    "print(f\"\\nAfter ReLU Activation (A1): {a1}\")\n",
    "\n",
    "#  SECOND HIDDEN LAYER (Sigmoid) \n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"LAYER 2 - SECOND HIDDEN LAYER (Sigmoid Activation)\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "W2 = [\n",
    "    [0.6, -0.2, 0.4],\n",
    "    [-0.3, 0.5, 0.7]\n",
    "]\n",
    "B2 = [0.1, -0.8]\n",
    "\n",
    "print(\"\\nWeights W2:\")\n",
    "for i, row in enumerate(W2):\n",
    "    print(f\"  Node {i+1}: {row}\")\n",
    "print(f\"Bias B2: {B2}\")\n",
    "\n",
    "hidden_layer_2.setup(a1, W2, B2)\n",
    "z2 = hidden_layer_2.calculate_weighted_sum()\n",
    "\n",
    "print(f\"\\nWeighted Sum (Z2):\")\n",
    "for i, val in enumerate(z2):\n",
    "    calculation = f\"({W2[i][0]}×{a1[0]:.4f}) + ({W2[i][1]}×{a1[1]:.4f}) + ({W2[i][2]}×{a1[2]:.4f}) + {B2[i]}\"\n",
    "    print(f\"  Node {i+1}: {val:.4f}\")\n",
    "    print(f\"    Calculation: {calculation}\")\n",
    "\n",
    "a2 = hidden_layer_2.activation('sigmoid')\n",
    "print(f\"\\nAfter Sigmoid Activation (A2):\")\n",
    "for i, val in enumerate(a2):\n",
    "    print(f\"  Node {i+1}: {val:.6f}\")\n",
    "\n",
    "#  OUTPUT LAYER (Sigmoid) \n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"LAYER 3 - OUTPUT LAYER (Sigmoid Activation)\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "W3 = [[0.7, -0.5]]\n",
    "B3 = [0.2]\n",
    "\n",
    "print(\"\\nWeights W3:\")\n",
    "for i, row in enumerate(W3):\n",
    "    print(f\"  Output Node: {row}\")\n",
    "print(f\"Bias B3: {B3}\")\n",
    "\n",
    "output_layer.setup(a2, W3, B3)\n",
    "z3 = output_layer.calculate_weighted_sum()\n",
    "\n",
    "print(f\"\\nWeighted Sum (Z3):\")\n",
    "calculation = f\"({W3[0][0]}×{a2[0]:.6f}) + ({W3[0][1]}×{a2[1]:.6f}) + {B3[0]}\"\n",
    "print(f\"  Output Node: {z3[0]:.4f}\")\n",
    "print(f\"    Calculation: {calculation}\")\n",
    "\n",
    "predictions = output_layer.activation('sigmoid')\n",
    "print(f\"\\nAfter Sigmoid Activation (Predictions): {predictions[0]:.6f}\")\n",
    "\n",
    "#  CALCULATE LOSS \n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"LOSS CALCULATION\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "loss = output_layer.calculate_loss(target_output, 'binary_crossentropy')\n",
    "\n",
    "print(f\"\\nPredicted Output: {predictions[0]:.6f}\")\n",
    "print(f\"Target Output: {target_output[0]}\")\n",
    "print(f\"Binary Cross Entropy Loss: {loss:.6f}\")\n",
    "\n",
    "#  FINAL CLASSIFICATION RESULT \n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"CLASSIFICATION RESULT\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "probability_malignant = predictions[0]\n",
    "probability_benign = 1 - probability_malignant\n",
    "threshold = 0.5\n",
    "\n",
    "print(f\"\\nProbability Malignant (1): {probability_malignant:.6f} ({probability_malignant*100:.2f}%)\")\n",
    "print(f\"Probability Benign (0): {probability_benign:.6f} ({probability_benign*100:.2f}%)\")\n",
    "print(f\"Decision Threshold: {threshold}\")\n",
    "\n",
    "if probability_malignant >= threshold:\n",
    "    prediction_class = \"MALIGNANT (1)\"\n",
    "else:\n",
    "    prediction_class = \"BENIGN (0)\"\n",
    "\n",
    "print(f\"\\nPredicted Classification: {prediction_class}\")\n",
    "print(f\"Confidence: {max(probability_malignant, probability_benign)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 75)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "School",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
